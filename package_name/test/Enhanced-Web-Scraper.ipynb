{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhanced web scraping...\n",
      "Scraping: https://mbc.studentlife.umich.edu\n",
      "Scraping: https://basicneeds.ucsb.edu\n",
      "Scraping: https://www.berkeley.edu/basicneeds/\n",
      "Scraping: https://basicneeds.ucsd.edu/\n",
      "Scraping: https://basicneeds.osu.edu/\n",
      "Error scraping https://basicneeds.osu.edu/: Message: unknown error: net::ERR_NAME_NOT_RESOLVED\n",
      "  (Session info: chrome=133.0.6943.142)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x00000001028bf708 chromedriver + 5969672\n",
      "1   chromedriver                        0x00000001028b732a chromedriver + 5935914\n",
      "2   chromedriver                        0x0000000102373650 chromedriver + 415312\n",
      "3   chromedriver                        0x000000010236a5e0 chromedriver + 378336\n",
      "4   chromedriver                        0x000000010235a7a8 chromedriver + 313256\n",
      "5   chromedriver                        0x000000010235c49d chromedriver + 320669\n",
      "6   chromedriver                        0x000000010235ab2b chromedriver + 314155\n",
      "7   chromedriver                        0x000000010235a553 chromedriver + 312659\n",
      "8   chromedriver                        0x000000010235a254 chromedriver + 311892\n",
      "9   chromedriver                        0x0000000102357fe0 chromedriver + 303072\n",
      "10  chromedriver                        0x000000010235889a chromedriver + 305306\n",
      "11  chromedriver                        0x0000000102376b29 chromedriver + 428841\n",
      "12  chromedriver                        0x0000000102413205 chromedriver + 1069573\n",
      "13  chromedriver                        0x00000001023eb172 chromedriver + 905586\n",
      "14  chromedriver                        0x00000001024124f7 chromedriver + 1066231\n",
      "15  chromedriver                        0x00000001023eaf43 chromedriver + 905027\n",
      "16  chromedriver                        0x00000001023b6cea chromedriver + 691434\n",
      "17  chromedriver                        0x00000001023b7e41 chromedriver + 695873\n",
      "18  chromedriver                        0x0000000102882910 chromedriver + 5720336\n",
      "19  chromedriver                        0x0000000102886850 chromedriver + 5736528\n",
      "20  chromedriver                        0x0000000102863c07 chromedriver + 5594119\n",
      "21  chromedriver                        0x00000001028872fb chromedriver + 5739259\n",
      "22  chromedriver                        0x0000000102852484 chromedriver + 5522564\n",
      "23  chromedriver                        0x00000001028a51d8 chromedriver + 5861848\n",
      "24  chromedriver                        0x00000001028a539f chromedriver + 5862303\n",
      "25  chromedriver                        0x00000001028b6f08 chromedriver + 5934856\n",
      "26  libsystem_pthread.dylib             0x00007ff80ad49259 _pthread_start + 125\n",
      "27  libsystem_pthread.dylib             0x00007ff80ad44c7b thread_start + 15\n",
      "\n",
      "Web scraping completed! Data saved to enhanced_college_basic_needs_data.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "\n",
    "# Load NLP model (helps categorize text automatically)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define university URLs to scrape\n",
    "university_urls = [\n",
    "    \"https://mbc.studentlife.umich.edu\",\n",
    "    \"https://basicneeds.ucsb.edu\",\n",
    "    \"https://www.berkeley.edu/basicneeds/\",\n",
    "    \"https://basicneeds.ucsd.edu/\",\n",
    "    \"https://basicneeds.osu.edu/\"\n",
    "]\n",
    "\n",
    "# Define keywords to extract relevant content\n",
    "keywords = [\n",
    "    \"Food Security\", \"Housing Stability\", \"Financial Assistance\", \"Healthcare Services\", \"Mental Health Support\",\n",
    "    \"Transportation Access\", \"Personal Care Items\", \"Childcare Support\", \"Technology Access\", \"Clothing & Weather Essentials\",\n",
    "    \"Academic Support\", \"Community & Belonging\", \"School Supplies\", \"Cooking Supplies\", \"Cleaning Supplies\",\n",
    "    \"Nutrition Education\", \"Financial Literacy\", \"Legal Support\", \"Crisis Intervention\", \"Laundry Access\",\n",
    "    \"Career Resources\", \"Substance Abuse Support\", \"Financial Counseling\", \"Emergency Housing\", \n",
    "    \"Immigration & International Student Support\", \"Communication Services\", \"Domestic Violence Resources\"\n",
    "]\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Initialize list to store extracted data\n",
    "data = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes excessive spaces, newlines, and special characters from text.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def extract_relevant_text(url):\n",
    "    \"\"\"Extracts relevant content based on predefined keywords.\"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    text = clean_text(soup.get_text().lower())\n",
    "    extracted_info = {\"URL\": url, \"Text\": text}\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        extracted_info[keyword] = \"Yes\" if keyword.lower() in text else \"No\"\n",
    "    \n",
    "    return extracted_info\n",
    "\n",
    "def extract_contact_info(text):\n",
    "    \"\"\"Extracts email and phone numbers from the scraped text.\"\"\"\n",
    "    emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "    phones = re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text)\n",
    "    return {\"Emails\": \", \".join(set(emails)), \"Phone Numbers\": \", \".join(set(phones))}\n",
    "\n",
    "def categorize_services(text):\n",
    "    \"\"\"Uses NLP to categorize extracted text into relevant service categories.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    categories = {key: \"No\" for key in keywords}\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        for key in keywords:\n",
    "            if key.lower() in sent.text.lower():\n",
    "                categories[key] = \"Yes\"\n",
    "    return categories\n",
    "\n",
    "print(\"Starting enhanced web scraping...\")\n",
    "for url in university_urls:\n",
    "    try:\n",
    "        print(f\"Scraping: {url}\")\n",
    "        extracted_data = extract_relevant_text(url)\n",
    "        contact_info = extract_contact_info(extracted_data[\"Text\"])\n",
    "        categorized_data = categorize_services(extracted_data[\"Text\"])\n",
    "        \n",
    "        # Merge all extracted data\n",
    "        final_data = {**extracted_data, **contact_info, **categorized_data}\n",
    "        data.append(final_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(columns=[\"Text\"], inplace=True)  # Remove raw text to keep CSV clean\n",
    "df.to_csv(\"enhanced_college_basic_needs_data.csv\", index=False)\n",
    "print(\"Web scraping completed! Data saved to enhanced_college_basic_needs_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
