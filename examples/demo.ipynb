{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a222f1",
   "metadata": {},
   "source": [
    "Before running the demo, download the spacy english model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397975f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a766c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhanced web scraping...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 159\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m university_urls:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;66;03m#print(f\"Scraping: {url}\")\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m         extracted_data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_relevant_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m         contact_info \u001b[38;5;241m=\u001b[39m extract_contact_info(extracted_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m#         categorized_data = categorize_services(extracted_data[\"Text\"])\u001b[39;00m\n\u001b[1;32m    162\u001b[0m         \n\u001b[1;32m    163\u001b[0m         \u001b[38;5;66;03m# Merge all extracted data and got rid of the categorized data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 115\u001b[0m, in \u001b[0;36mextract_relevant_text\u001b[0;34m(url, limit)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extracts relevant content and retrieves keyword occurrences with sentence context.\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m--> 115\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    117\u001b[0m text \u001b[38;5;241m=\u001b[39m clean_text(soup\u001b[38;5;241m.\u001b[39mget_text())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "\n",
    "\n",
    "def generate_url_list(school_info, max_links=20):\n",
    "    \"\"\"\n",
    "    This function returns the subdomain links visible from a food bank or wellness programs homepage.\n",
    "    Params:\n",
    "        school_info: DataFrame with 'school_name' and 'url' columns\n",
    "        max_links: max size of the list being returned for each school\n",
    "    Returns:\n",
    "        result_df: DataFrame with 'school_name' and 'url' columns\n",
    "    \"\"\"\n",
    "    all_links = []  \n",
    "\n",
    "    \n",
    "    for index, row in school_info.iterrows():\n",
    "        #get school name and url to base/starting page\n",
    "        school_name = row[\"school_name\"] \n",
    "        url = row[\"url\"]  \n",
    "\n",
    "        \n",
    "        driver = webdriver.Chrome() \n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # parsing url to ensure consistency and proper formatting\n",
    "        parsed_url = urlparse(url)\n",
    "        #takes elements such as scheme and netloc to create valid base domain\n",
    "        base_domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "\n",
    "        # set data structure used to avoid duplicates\n",
    "        links = set()\n",
    "\n",
    "        #looping though each sublink\n",
    "        for a in driver.find_elements(By.TAG_NAME, \"a\"):\n",
    "            href = a.get_attribute(\"href\")\n",
    "            if href: # if link exists\n",
    "                # joining to ensure only focused websites are being generated\n",
    "                full_link = urljoin(base_domain, href)\n",
    "                #adding to list of links if it has base domain \n",
    "                if full_link.startswith(base_domain) and full_link not in links:\n",
    "                    links.add(full_link)\n",
    "                    if len(links) >= max_links: #stopping point after max_links\n",
    "                        break\n",
    "\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        # all links for a school will have school name but different urls\n",
    "        for link in links:\n",
    "            all_links.append({\"school_name\": school_name, \"url\": link})\n",
    "\n",
    "    # conver to dataframe \n",
    "    result_df = pd.DataFrame(all_links)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "# small example of dataframe for 5 schools\n",
    "school_info = pd.DataFrame({\n",
    "    \"school_name\": [\"UNC\", \"UGA\",\"UC Davis\", \"UCLA\", \"PSU\"], \n",
    "    \"url\": [\"https://dos.unc.edu/student-support/basicneeds/\",\n",
    "            \"https://well-being.uga.edu/basic-needs/\",\n",
    "            \"https://financialaid.ucdavis.edu/wellness-outreach/basic-needs\",\n",
    "            \"https://bewellbruin.ucla.edu/resource/ucla-basic-needs\",\n",
    "            \"https://studentaffairs.psu.edu/basic-needs-support\"\n",
    "            ]  \n",
    "})\n",
    "\n",
    "\n",
    "result = generate_url_list(school_info)\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Urls and school names list \n",
    "university_urls = result[\"url\"]\n",
    "school_names = result[\"school_name\"]\n",
    "\n",
    "\n",
    "keywords = [\n",
    "    \"Food Security\", \"Housing Stability\", \"Financial Assistance\", \"Healthcare Services\", \"Mental Health Support\",\n",
    "    \"Transportation Access\", \"Personal Care Items\", \"Childcare Support\", \"Technology Access\", \"Clothing & Weather Essentials\",\n",
    "    \"Academic Support\", \"Community & Belonging\", \"School Supplies\", \"Cooking Supplies\", \"Cleaning Supplies\",\n",
    "    \"Nutrition Education\", \"Financial Literacy\", \"Legal Support\", \"Crisis Intervention\", \"Laundry Access\",\n",
    "    \"Career Resources\", \"Substance Abuse Support\", \"Financial Counseling\", \"Emergency Housing\", \n",
    "    \"Immigration & International Student Support\", \"Communication Services\", \"Domestic Violence Resources\"\n",
    "]\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "data = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes excessive spaces, newlines, and special characters from text.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "#updated extraction function\n",
    "def extract_relevant_text(url, limit = 10):\n",
    "    \"\"\"Extracts relevant content and retrieves keyword occurrences with sentence context.\"\"\"\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    text = clean_text(soup.get_text())\n",
    "    extracted_info = {\"URL\": url, \"Text\": text}\n",
    "\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]  # Tokenize into sentences\n",
    "    total_occurrences = []\n",
    "    #loop through the keyword list\n",
    "    for keyword in keywords:\n",
    "        keyword_lower = keyword.lower()\n",
    "        occurrences = []\n",
    "        \n",
    "        # Find occurences\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if keyword_lower in sentence.lower():  # If the keyword is found in the sentence\n",
    "                before = sentences[i - 1] if i > 0 else \" \"  # Previous sentence\n",
    "                after = sentences[i + 1] if i < len(sentences) - 1 else \"N/A\"  # Next sentence\n",
    "                highlighted_sentence = sentence.replace(keyword, keyword.upper())\n",
    "                occurrence_text = f\"Occurrence X: {before} {highlighted_sentence} {after} \\n\"\n",
    "                occurrences.append(occurrence_text)\n",
    "                total_occurrences.append(occurrence_text)\n",
    "            if len(total_occurrences) >= 5:\n",
    "                break\n",
    "        # Placing the occurences into the same column separated by ||\n",
    "        extracted_info[keyword] = \"\\n\".join(occurrences) if occurrences else \"No\"\n",
    "        if len(total_occurrences) >= 5:\n",
    "            break\n",
    "\n",
    "    return extracted_info\n",
    "\n",
    "\n",
    "def extract_contact_info(text):\n",
    "    \"\"\"Extracts email and phone numbers from the scraped text.\"\"\"\n",
    "    emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "    phones = re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text)\n",
    "    return {\"Emails\": \", \".join(set(emails)), \"Phone Numbers\": \", \".join(set(phones))}\n",
    "\n",
    "\n",
    "print(\"Starting enhanced web scraping...\")\n",
    "for url in university_urls:\n",
    "    try:\n",
    "        #print(f\"Scraping: {url}\")\n",
    "        extracted_data = extract_relevant_text(url)\n",
    "        contact_info = extract_contact_info(extracted_data[\"Text\"])\n",
    "#         categorized_data = categorize_services(extracted_data[\"Text\"])\n",
    "        \n",
    "        # Merge all extracted data and got rid of the categorized data\n",
    "        final_data = {**extracted_data, **contact_info}  #, **categorized_data\n",
    "        data.append(final_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(columns=[\"Text\"], inplace=True) \n",
    "df[\"school_name\"] = result[\"school_name\"] # Remove raw text to keep CSV clean\n",
    "\n",
    "#df\n",
    "\n",
    "\n",
    "#Aggregate keyword occurrences into a single row per school\n",
    "def merge_occurrences(series):\n",
    "    \"\"\"Merge occurrences from multiple sublinks, ensuring proper sequencing.\"\"\"\n",
    "    unique_values = series.dropna().unique()\n",
    "    filtered_values = [val for val in unique_values if val != \"No\"]\n",
    "\n",
    "    if not filtered_values:\n",
    "        return \"No\"\n",
    "\n",
    "    # Step 1: Standardize occurrence format (replace numbers with 'X')\n",
    "    occurrences = \"\\n\".join(filtered_values)\n",
    "    occurrence_list = [line for line in occurrences.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # Step 3: Renumber properly\n",
    "    reordered = []\n",
    "    for i, occ in enumerate(occurrence_list):\n",
    "        reordered.append(occ.replace(\"Occurrence X:\", f\"Occurrence {i + 1}:\") + \"\\n\")\n",
    "\n",
    "    return \"\\n\".join(reordered)\n",
    "\n",
    "\n",
    "#Count total mentions per school\n",
    "def count_mentions(series):\n",
    "    \"\"\"Count total keyword mentions across multiple rows for a school.\"\"\"\n",
    "    return series.str.count(\"Occurrence\").sum()\n",
    "\n",
    "#Perform groupby aggregation\n",
    "agg_dict = {keyword: merge_occurrences for keyword in keywords}\n",
    "agg_dict[\"Emails\"] = merge_occurrences\n",
    "agg_dict[\"Phone Numbers\"] = merge_occurrences\n",
    "\n",
    "df_grouped = df.groupby(\"school_name\").agg(agg_dict).reset_index()\n",
    "\n",
    "#Create a new column for total keyword mentions\n",
    "df_grouped[\"Total Mentions\"] = df[keywords].applymap(lambda x: x.count(\"Occurrence\") if isinstance(x, str) else 0).groupby(df[\"school_name\"]).sum().sum(axis=1).values\n",
    "\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ee61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"first_word_search.csv\", index=False)\n",
    "df_grouped.to_csv(\"condensed_word_search.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05cbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
