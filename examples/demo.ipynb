{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "from Uniscraper.Uniscraper import uniscraper\n",
    "\n",
    "\n",
    "def generate_url_list(school_info, max_links=20):\n",
    "    \"\"\"\n",
    "    This function returns the subdomain links visible from a food bank or wellness programs homepage.\n",
    "    Params:\n",
    "        school_info: DataFrame with 'school_name' and 'url' columns\n",
    "        max_links: max size of the list being returned for each school\n",
    "    Returns:\n",
    "        result_df: DataFrame with 'school_name' and 'url' columns\n",
    "    \"\"\"\n",
    "    all_links = []  \n",
    "\n",
    "    \n",
    "    for index, row in school_info.iterrows():\n",
    "        #get school name and url to base/starting page\n",
    "        school_name = row[\"school_name\"] \n",
    "        url = row[\"url\"]  \n",
    "\n",
    "        \n",
    "        driver = webdriver.Chrome() \n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # parsing url to ensure consistency and proper formatting\n",
    "        parsed_url = urlparse(url)\n",
    "        #takes elements such as scheme and netloc to create valid base domain\n",
    "        base_domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "\n",
    "        # set data structure used to avoid duplicates\n",
    "        links = set()\n",
    "\n",
    "        #looping though each sublink\n",
    "        for a in driver.find_elements(By.TAG_NAME, \"a\"):\n",
    "            href = a.get_attribute(\"href\")\n",
    "            if href: # if link exists\n",
    "                # joining to ensure only focused websites are being generated\n",
    "                full_link = urljoin(base_domain, href)\n",
    "                #adding to list of links if it has base domain \n",
    "                if full_link.startswith(base_domain) and full_link not in links:\n",
    "                    links.add(full_link)\n",
    "                    if len(links) >= max_links: #stopping point after max_links\n",
    "                        break\n",
    "\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        # all links for a school will have school name but different urls\n",
    "        for link in links:\n",
    "            all_links.append({\"school_name\": school_name, \"url\": link})\n",
    "\n",
    "    # conver to dataframe \n",
    "    result_df = pd.DataFrame(all_links)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "# small example of dataframe for 5 schools\n",
    "school_info = pd.DataFrame({\n",
    "    \"school_name\": [\"UNC\", \"UGA\",\"UC Davis\", \"UCLA\", \"PSU\"], \n",
    "    \"url\": [\"https://dos.unc.edu/student-support/basicneeds/\",\n",
    "            \"https://well-being.uga.edu/basic-needs/#:~:text=YOUR%20BASIC%20WELL-BEING%20NEEDS&text=Access%20to%20essential%20resources%20is,to%20students%20at%20no%20cost\",\n",
    "            \"https://financialaid.ucdavis.edu/wellness-outreach/basic-needs\",\n",
    "            \"https://bewellbruin.ucla.edu/resource/ucla-basic-needs\",\n",
    "            \"https://studentaffairs.psu.edu/basic-needs-support\"\n",
    "            ]  \n",
    "})\n",
    "\n",
    "result = generate_url_list(school_info)\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Urls and school names list \n",
    "university_urls = result[\"url\"]\n",
    "school_names = result[\"school_name\"]\n",
    "\n",
    "\n",
    "keywords = [\n",
    "    \"Food Security\", \"Housing Stability\", \"Financial Assistance\", \"Healthcare Services\", \"Mental Health Support\",\n",
    "    \"Transportation Access\", \"Personal Care Items\", \"Childcare Support\", \"Technology Access\", \"Clothing & Weather Essentials\",\n",
    "    \"Academic Support\", \"Community & Belonging\", \"School Supplies\", \"Cooking Supplies\", \"Cleaning Supplies\",\n",
    "    \"Nutrition Education\", \"Financial Literacy\", \"Legal Support\", \"Crisis Intervention\", \"Laundry Access\",\n",
    "    \"Career Resources\", \"Substance Abuse Support\", \"Financial Counseling\", \"Emergency Housing\", \n",
    "    \"Immigration & International Student Support\", \"Communication Services\", \"Domestic Violence Resources\"\n",
    "]\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "data = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes excessive spaces, newlines, and special characters from text.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def extract_relevant_text(url):\n",
    "    \"\"\"Extracts relevant content based on predefined keywords.\"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    text = clean_text(soup.get_text().lower())\n",
    "    extracted_info = {\"URL\": url, \"Text\": text}\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        extracted_info[keyword] = \"Yes\" if keyword.lower() in text else \"No\"\n",
    "    \n",
    "    return extracted_info\n",
    "\n",
    "def extract_contact_info(text):\n",
    "    \"\"\"Extracts email and phone numbers from the scraped text.\"\"\"\n",
    "    emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "    phones = re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text)\n",
    "    return {\"Emails\": \", \".join(set(emails)), \"Phone Numbers\": \", \".join(set(phones))}\n",
    "\n",
    "def categorize_services(text):\n",
    "    \"\"\"Uses NLP to categorize extracted text into relevant service categories.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    categories = {key: \"No\" for key in keywords}\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        for key in keywords:\n",
    "            if key.lower() in sent.text.lower():\n",
    "                categories[key] = \"Yes\"\n",
    "    return categories\n",
    "\n",
    "print(\"Starting enhanced web scraping...\")\n",
    "for url in university_urls:\n",
    "    try:\n",
    "        #print(f\"Scraping: {url}\")\n",
    "        extracted_data = extract_relevant_text(url)\n",
    "        contact_info = extract_contact_info(extracted_data[\"Text\"])\n",
    "        categorized_data = categorize_services(extracted_data[\"Text\"])\n",
    "        \n",
    "        # Merge all extracted data\n",
    "        final_data = {**extracted_data, **contact_info, **categorized_data}\n",
    "        data.append(final_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(columns=[\"Text\"], inplace=True) \n",
    "df[\"school_name\"] = result[\"school_name\"] # Remove raw text to keep CSV clean\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
