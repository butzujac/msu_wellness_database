{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a766c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Service\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "from Uniscraper.Uniscraper import uniscraper\n",
    "\n",
    "\n",
    "def generate_url_list(school_info, max_links=20):\n",
    "    \"\"\"\n",
    "    This function returns the subdomain links visible from a food bank or wellness programs homepage.\n",
    "    Params:\n",
    "        school_info: DataFrame with 'school_name' and 'url' columns\n",
    "        max_links: max size of the list being returned for each school\n",
    "    Returns:\n",
    "        result_df: DataFrame with 'school_name' and 'url' columns\n",
    "    \"\"\"\n",
    "    all_links = []  \n",
    "\n",
    "    \n",
    "    for index, row in school_info.iterrows():\n",
    "        #get school name and url to base/starting page\n",
    "        school_name = row[\"school_name\"] \n",
    "        url = row[\"url\"]  \n",
    "\n",
    "        \n",
    "        driver = webdriver.Chrome() \n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # parsing url to ensure consistency and proper formatting\n",
    "        parsed_url = urlparse(url)\n",
    "        #takes elements such as scheme and netloc to create valid base domain\n",
    "        base_domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "\n",
    "        # set data structure used to avoid duplicates\n",
    "        links = set()\n",
    "\n",
    "        #looping though each sublink\n",
    "        for a in driver.find_elements(By.TAG_NAME, \"a\"):\n",
    "            href = a.get_attribute(\"href\")\n",
    "            if href: # if link exists\n",
    "                # joining to ensure only focused websites are being generated\n",
    "                full_link = urljoin(base_domain, href)\n",
    "                #adding to list of links if it has base domain \n",
    "                if full_link.startswith(base_domain) and full_link not in links:\n",
    "                    links.add(full_link)\n",
    "                    if len(links) >= max_links: #stopping point after max_links\n",
    "                        break\n",
    "\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        # all links for a school will have school name but different urls\n",
    "        for link in links:\n",
    "            all_links.append({\"school_name\": school_name, \"url\": link})\n",
    "\n",
    "    # conver to dataframe \n",
    "    result_df = pd.DataFrame(all_links)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "# small example of dataframe for 5 schools\n",
    "school_info = pd.DataFrame({\n",
    "    \"school_name\": [\"UNC\", \"UGA\"], #,\"UC Davis\", \"UCLA\", \"PSU\"], \n",
    "    \"url\": [\"https://dos.unc.edu/student-support/basicneeds/\",\n",
    "            \"https://well-being.uga.edu/basic-needs/#:~:text=YOUR%20BASIC%20WELL-BEING%20NEEDS&text=Access%20to%20essential%20resources%20is,to%20students%20at%20no%20cost\",\n",
    "            #\"https://financialaid.ucdavis.edu/wellness-outreach/basic-needs\",\n",
    "            #\"https://bewellbruin.ucla.edu/resource/ucla-basic-needs\",\n",
    "            #\"https://studentaffairs.psu.edu/basic-needs-support\"\n",
    "            ]  \n",
    "})\n",
    "\n",
    "result = generate_url_list(school_info)\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Urls and school names list \n",
    "university_urls = result[\"url\"]\n",
    "school_names = result[\"school_name\"]\n",
    "\n",
    "\n",
    "keywords = [\n",
    "    \"Food Security\", \"Housing Stability\", \"Financial Assistance\", \"Healthcare Services\", \"Mental Health Support\",\n",
    "    \"Transportation Access\", \"Personal Care Items\", \"Childcare Support\", \"Technology Access\", \"Clothing & Weather Essentials\",\n",
    "    \"Academic Support\", \"Community & Belonging\", \"School Supplies\", \"Cooking Supplies\", \"Cleaning Supplies\",\n",
    "    \"Nutrition Education\", \"Financial Literacy\", \"Legal Support\", \"Crisis Intervention\", \"Laundry Access\",\n",
    "    \"Career Resources\", \"Substance Abuse Support\", \"Financial Counseling\", \"Emergency Housing\", \n",
    "    \"Immigration & International Student Support\", \"Communication Services\", \"Domestic Violence Resources\"\n",
    "]\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "data = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes excessive spaces, newlines, and special characters from text.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "#updated extraction function\n",
    "def extract_relevant_text(url):\n",
    "    \"\"\"Extracts relevant content and retrieves keyword occurrences with sentence context.\"\"\"\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    text = uniscraper(url)\n",
    "    #soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    #text = clean_text(soup.get_text().lower())\n",
    "    extracted_info = {\"URL\": url, \"Text\": text}\n",
    "\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]  # Tokenize into sentences\n",
    "\n",
    "    #loop through the keyword list\n",
    "    for keyword in keywords:\n",
    "        keyword_lower = keyword.lower()\n",
    "        occurrences = []\n",
    "        \n",
    "        # Find occurences\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if keyword_lower in sentence.lower():  # If the keyword is found in the sentence\n",
    "                before = sentences[i - 1] if i > 0 else \"N/A\"  # Previous sentence\n",
    "                after = sentences[i + 1] if i < len(sentences) - 1 else \"N/A\"  # Next sentence\n",
    "                occurrence_text = f\"Occurrence {len(occurrences) + 1}: Before: {before} | Within: {sentence} | After: {after}\"\n",
    "                occurrences.append(occurrence_text)\n",
    "\n",
    "        # Placing the occurences into the same column separated by ||\n",
    "        extracted_info[keyword] = \" || \".join(occurrences) if occurrences else \"No\"\n",
    "\n",
    "    return extracted_info\n",
    "\n",
    "\n",
    "# Old extraction function\n",
    "# def extract_relevant_text(url):\n",
    "#     \"\"\"Extracts relevant content based on predefined keywords.\"\"\"\n",
    "#     driver.get(url)\n",
    "#     time.sleep(3)\n",
    "#     soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#     text = clean_text(soup.get_text().lower())\n",
    "#     extracted_info = {\"URL\": url, \"Text\": text}\n",
    "    \n",
    "#     for keyword in keywords:\n",
    "#         extracted_info[keyword] = \"Yes\" if keyword.lower() in text else \"No\"\n",
    "    \n",
    "#     return extracted_info\n",
    "\n",
    "def extract_contact_info(text):\n",
    "    \"\"\"Extracts email and phone numbers from the scraped text.\"\"\"\n",
    "    emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "    phones = re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text)\n",
    "    return {\"Emails\": \", \".join(set(emails)), \"Phone Numbers\": \", \".join(set(phones))}\n",
    "\n",
    "# old categorize function\n",
    "# def categorize_services(text):\n",
    "#     \"\"\"Uses NLP to categorize extracted text into relevant service categories.\"\"\"\n",
    "#     doc = nlp(text)\n",
    "#     categories = {key: \"No\" for key in keywords}\n",
    "    \n",
    "#     for sent in doc.sents:\n",
    "#         for key in keywords:\n",
    "#             if key.lower() in sent.text.lower():\n",
    "#                 categories[key] = \"Yes\"\n",
    "#     return categories\n",
    "\n",
    "print(\"Starting enhanced web scraping...\")\n",
    "for url in university_urls:\n",
    "    try:\n",
    "        #print(f\"Scraping: {url}\")\n",
    "        extracted_data = extract_relevant_text(url)\n",
    "        contact_info = extract_contact_info(extracted_data[\"Text\"])\n",
    "#         categorized_data = categorize_services(extracted_data[\"Text\"])\n",
    "        \n",
    "        # Merge all extracted data and got rid of the categorized data\n",
    "        final_data = {**extracted_data, **contact_info}  #, **categorized_data\n",
    "        data.append(final_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(columns=[\"Text\"], inplace=True) \n",
    "df[\"school_name\"] = result[\"school_name\"] # Remove raw text to keep CSV clean\n",
    "\n",
    "#df\n",
    "\n",
    "\n",
    "#Aggregate keyword occurrences into a single row per school\n",
    "def merge_occurrences(series):\n",
    "    \"\"\"Merge occurrences from multiple rows into a single row, separated by ' || '.\"\"\"\n",
    "    return \" || \".join(series.dropna().unique()) if not series.isnull().all() else \"No\"\n",
    "\n",
    "#Count total mentions per school\n",
    "def count_mentions(series):\n",
    "    \"\"\"Count total keyword mentions across multiple rows for a school.\"\"\"\n",
    "    return series.str.count(\"Occurrence\").sum()\n",
    "\n",
    "#Perform groupby aggregation\n",
    "agg_dict = {keyword: merge_occurrences for keyword in keywords}\n",
    "agg_dict[\"Emails\"] = merge_occurrences\n",
    "agg_dict[\"Phone Numbers\"] = merge_occurrences\n",
    "\n",
    "df_grouped = df.groupby(\"school_name\").agg(agg_dict).reset_index()\n",
    "\n",
    "#Create a new column for total keyword mentions\n",
    "df_grouped[\"Total Mentions\"] = df[keywords].applymap(lambda x: x.count(\"Occurrence\") if isinstance(x, str) else 0).groupby(df[\"school_name\"]).sum().sum(axis=1).values\n",
    "\n",
    "df_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ee61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"first_word_search.csv\", index=False)\n",
    "df_grouped.to_csv(\"condensed_word_search.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05cbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
